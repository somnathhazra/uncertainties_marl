run: "dist"

# use epsilon greedy action selector
action_selector: "epsilon_greedy"
epsilon_start: 1.0
epsilon_finish: 0.05
epsilon_anneal_time: 100000 # 50000

runner: "parallel_dist"
batch_size_run: 2
buffer_size: 5000
batch_size: 64 # 128

# t_max: 10050000

# update the target network every {} episodes
target_update_interval: 200

# use the Q_Learner to train
agent_output_type: "q"
mac: "n_iql_dist_mac"
learner: "iqn_dist_learner"
double_q: True # False
mixer: "dplex" # "dmix"
mixing_embed_dim: 32
hypernet_layers: 2
hypernet_embed: 64
td_lambda: 0.6
lr: 0.001 # Learning rate for agents
q_lambda: False

num_kernel: 4
adv_hypernet_layers: 1
adv_hypernet_embed: 64
is_minus_one: True
weighted_head: True
is_adv_attention: True
is_stop_gradient: True

hpn_hyper_dim: 32
hpn_hyper_activation: 'relu'
hpn_head_num: 2

name: "dmix_bf"

# IQN specific parameters
agent: "iqn_rnn_dist"
sort_quantiles: True
optimizer: "Adam" # follow the optimizer used in the IQN paper
quantile_embed_dim: 64 
n_quantiles: 32 
n_target_quantiles: 32 
n_approx_quantiles: 32 

obs_agent_id: True # Include the agent's one_hot id in the observation
obs_last_action: True # Include the agent's last action (one_hot) in the observation
